---
title             : "`BFpack`: Flexible Bayes Factor Testing of Scientific Theories in `R` (Applications Only)"
shorttitle        : "Bayesian Hypothesis Testing using `BFpack`"

author: 
  - name          : "Joris Mulder"
    affiliation   : "Tilburg University, Jheronimus Academy of Data Science"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "j.mulder3@tilburguniversity.edu"
  - name          : "Donald R. Williams"
    affiliation   : "UC Davis" 
  - name          : "Xin Gu"
    affiliation   : "East China Normal University"
  - name          : "Anton Olsson-Collentine"
    affiliation   : "Tilburg University"
  - name          : "Andrew Tomarken"
    affiliation   : "Vanderbilt University"
  - name          : "Florian BoÌˆing-Messing"
    affiliation   : "Jheronimus Academy of Data Science"
  - name          : "Herbert Hoijtink"
    affiliation   : "Utrecht University"
  - name          : "Marlyne Meijerink"
    affiliation   : "Tilburg University"
  - name          : "Robbie van Aert"
    affiliation   : "Tilburg University"
  - name          : "Janosch Menke"
    affiliation   : "Utrecht University"
  - name          : "Jean-Paul Fox"
    affiliation   : "University of Twente"
  - name          : "Yves Rosseel"
    affiliation   : "Ghent University"
  - name          : "Eric-Jan Wagenmakers"
    affiliation   : "University of Amsterdam"
  - name          : "Caspar van Lissa"
    affiliation   : "Utrecht University"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"
  - id            : "2"
    institution   : "East China Normal University"
  - id            : "3"
    institution   : "Vanderbilt University"
  - id            : "4"
    institution   : "Jheronimus Academy of Data Science"
  - id            : "5"
    institution   : "Utrecht University"
  - id            : "6"
    institution   : "University of California Davis"
  - id            : "7"
    institution   : "University of Twente"
  - id            : "8"
    institution   : "Ghent University"
  - id            : "9"
    institution   : "University of Amsterdam"

abstract: |
  There has been a considerable methodological development of Bayes factors for hypothesis testing in the social and behavioral sciences, and related fields. This development is due to the flexibility of the Bayes factor for testing multiple hypotheses simultaneously, the ability to test complex hypotheses involving equality as well as order constraints on the parameters of interest, and the interpretability of the outcome as the weight of evidence provided by the data in support of competing scientific theories. The available software tools for Bayesian hypothesis testing are still limited however. In this paper we present a new `R`-package called `BFpack` that contains functions for Bayes factor hypothesis testing for the many common testing problems. The software includes novel tools for (i) Bayesian exploratory testing (null vs positive vs negative effects), (ii) Bayesian confirmatory testing (competing hypotheses with equality and/or order constraints), (iii) testing parameters such as means, regression coefficients, measures of association, and variance components under multivariate normal linear models, generalized linear (mixed) models, and survival models, (iv) automatic prior specification, and (v) allows the analysis of data that includes missing values that are missing at random.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
#output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(papaja)
library(knitr)
library(BFpack)
```


# Applications

This section presents a variety of testing problems that can be executed using `R` package `BFpack`.


## 1. Bayesian $t$ testing in medical research
The one-sample $t$ is one of the most popular tests in applied research for testing the mean $\mu$ in a normally distributed population, $N(\mu,\sigma^2)$, where $\sigma^2$ is the unknown variance, which is a nuisance parameter here. %For a two-samples $t$ test the difference of the two means is of key interest, i.e., $\mu_1-\mu_2$, where $\mu_1$ and $\mu_2$ denote the means of two normal populations, $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2)$, and $\sigma^2_1$ and $\sigma^2_2$ denote the unknown population variances which can equal or not depending on the context.
The standard exploratory one-sample $t$ test in `BFpack` is $H_1:\mu=\mu_0$ versus $H_2:\mu<\mu_0$ versus $H_3:\mu>\mu_0$, where $\mu_0$ is a pre-specified null value. Default Bayes factors are executed based on adjusted fractional Bayes methodology using a minimal fractional (e.g., $\frac{2}{n}$ for a one-sample $t$ test for a sample of size $n$) and a default fractional prior that is centered around the null value $\mu_0$. For technical details about the methodology, we refer the interested reader to Section A.1.

In this section we consider a confirmatory one-sided one-sample $t$ test that was discussed in \citet[][p. 196]{Howell:2012}, and originally presented in \cite{Rosa:1998}. An experiment was conducted to investigate whether practitioners of the therapeutic touch (a widely used nursing practice) can effectively identify which of their hands is below the experimenter's under blinded condition. Twenty-eight practitioners were involved and tested 10 times in the experiment. Researchers expected an average of 5 correct answers from each practitioner as it is the number by chance if they do not outperform others. In this example, the data are the number of correct answers from 0 to 10 of $n=28$ practitioners. The null and alternative hypotheses are $H_1:\mu=5$ and $H_2:\mu > 5$ where $\mu$ is the mean of the data. If $H_1:\mu=5$ is true, it means that practitioners give correct answers by chance, whereas if $H_2:\mu > 5$, this implies that practitioners do better than expected by random chance. In this test the complement hypothesis would be $H_3:\mu < 5$, which assumes that practitioners do worse than expected by chance. As there is virtually no reason that $H_3$ could be true, the complement hypothesis is excluded from the analysis.

First a standard classical $t$ test is executed, and then the output of this analysis is plugged into the `BF` function where hypotheses are formulated on the key parameter `mu`:
```{r, eval = FALSE}
install.packages("BFpack")
install.packages("bain")
library(BFpack)
library(bain)
ttest1 <- t_test(therapeutic, alternative = "greater", mu = 5)
print(ttest1)
BF1 <- BF(ttest1, hypothesis = "mu = 5; mu > 5", complement = FALSE)
print(BF1)
```
The first four lines install and load the `R` package `BFpack` and `bain`. `bain` contains a function for a standard $t$ test, `t_test`, which is equivalent to `t.test` function but also contains the sample size(s) and sample variance(s) of the data (needed for `BF`) in the output.
In the 5th line, `t_test` function renders a classical right-tailed one-sided $t$ test and stores the result in object `ttest1`, which after printing in the 6th line yields the $t$ value, degree of freedom, and $p$ value, as well as $95\%$ confidence interval:
<!-- Output of t-test here>
data:  therapeutic
t = -1.9318, df = 27, p-value = 0.968
alternative hypothesis: true mean is greater than 5
95 percent confidence interval:
 3.857523      Inf
sample estimates:
mean of x 
 4.392857 
-->
As the $p$ value is larger than the significance level of 0.05, we would not reject the traditional null hypothesis where $\mu$ equals 5 against a right-tailed alternative. This result however does not imply there is evidence in favor of $H_1:\mu=5$ as significance tests cannot be used for quantifying evidence for a null hypothesis. Next, the object `ttest1` is used as the input for the `BF` function for a Bayesian $t$ test in the 7th line. This executes a confirmatory one-sided Bayes factor test where the hypothesis $H_1: \mu=5$ is tested against $H_2: \mu> 5$. The argument `complement = FALSE` is used to exclude the complement hypothesis from the analysis. Equal prior probabilities for $H_1$ and $H_2$ are used by default, which implies that the two hypotheses are assumed to be equally likely a priori. The statement on the 8th line prints the Bayes factors and posterior probabilities for the formulated hypotheses which gives:
<!--
Call:
BF.t_test(x = ttest1, hypothesis = "mu = 5; mu > 5", complement = FALSE)

Bayesian hypothesis test
Type: Confirmatory
Object: t_test
Parameter: means
Method: generalized adjusted fractional Bayes factor

Posterior probabilities:
   Pr(hypothesis|data)
H1               0.943
H2               0.057

Evidence matrix:
      H1     H2
H1 1.000 16.473
H2 0.061  1.000

Hypotheses:
H1: mu=5
H2: mu>5
-->
%The results of the exploratory tests show that the posterior probabilities of the precise null ($\mu=5$), a negative effect ($\mu<5$), and a positive effect ($\mu>5$) are 0.345, 0.634, and 0.021, respectively, while assuming equal prior probabilities for the three hypotheses, i.e., $P(H_1)=P(H_2)=P(H_3)=\frac{1}{3}$. The results from the exploratory test show that the presence of a negative is most plausible given the observed data but the evidence is relatively small as there is still a probability of 0.345 that the precise null is true, and a small probability of 0.021 that there is a positive population effect.

The `Posterior probabilities` indicate that there is most evidence that a therapeutic touch does not exist ($H_1$) with a posterior probability of 0.943. Next, the `Evidence matrix` presents the Bayes factors between all hypotheses that are specified using the `hypothesis argument.` As the prior probabilities of hypothesis $H_1$ and $H_2$ are equal the Bayes factor of $H_1$ against $H_2$ is equal to the posterior odds of these hypotheses, $\frac{.943}{.057}=16.473$ (the small difference is caused by a rounding error in the last step). Further notice that the Bayes factor of $H_2$ against $H_1$ is equal to the reciprocal of the Bayes factor of $H_1$ against $H_2$ ($16.473=1/0.061$). This follows from the symmetry property of the Bayes factor, e.g., if the relative evidence for $H_1$ against $H_2$ is about 16.473, then the relative evidence for $H_2$ against $H_1$ is about $\frac{1}{16.473}$. Finally the tested hypotheses are printed under.

By using the `summary` function, a more comprehensive overview of the results is printed, which includes the four quantities in the extended Savage-Dickey density ratio in Equation 4 for each hypothesis, as well as the results of the standard exhaustive test, $H_1: \mu=5$ versus $H_2: \mu<5$ versus $H_3: \mu>5$, assuming equal prior probabilities.
```
summary(BF1)
```
This results in the following additional output for the confirmatory test
<!--
Specification table:
   complex= complex>  fit=  fit>   BF=   BF>    BF   PHP
H1    0.195      1.0 0.205 1.000 1.053 1.000 1.053 0.943
H2    1.000      0.5 1.000 0.032 1.000 0.064 0.064 0.057
-->
Figure 1 displays the unconstrained posterior and unconstrained fractional prior for these data. The relative complexity of the one-sided hypothesis $H_2$, in the column `complex>`, is equal to the prior probability that the constraints hold under the unconstrained model, which is equal to .5 (as the prior is centered at the test value of 5; Figure 1, dashed line). This value quantifies the relative size of the constrained parameter space as the one-sided hypothesis covers half of the parameter space. Furthermore the relative fit of the one-sided hypothesis, in the column `fit>`, quantifies the relative fit of the one-sided constraint, which is calculated as the posterior probability that the constraint hold, which is equal to .032 (Figure 1, solid line). Furthermore, the relative complexity and the relative fit of the equality hypothesis, in the column `complex=` and `fit=`, respectively, are equal to the probability density at $\mu=5$ for the unconstrained fractional prior and the unconstrained posterior (Figure 1). This may help users to understand how the Bayes factors using Equation 4 are computed. 

<!-- \begin{figure} -->
<!-- \centering -->
<!-- \includegraphics[height=6cm,keepaspectratio=true]{ttest.pdf} -->
<!-- \caption{Unconstrained posterior based on the \code{therapeutic} data (solid line), and the implied unconstrained fractional prior (dashed line). The posterior probability that $\mu>5$ holds is equal to .032 and the prior probability is equal to .5. The posterior and prior density at $\mu=5$ are 0.205 and 0.195, respectively.} -->
<!-- \label{fig1} -->
<!-- \end{figure} -->

The `summary(BF1)` statement also provides the posterior probabilities for an exploratory (exhaustive) test, $H_1:\mu=5$ versus $H_2:\mu<5$ versus $H_3:\mu>5$ assuming equal prior probabilities (i.e., $\text{Pr}(H_1)=\text{Pr}(H_2)=\text{Pr}(H_3)=\frac{1}{3}$), which yields:
<!--
Bayesian hypothesis test
Type: Exploratory
Object: t_test
Parameter: means
Method: generalized adjusted fractional Bayes factor

Posterior probabilities:
   Pr(=5) Pr(<5) Pr(>5)
mu  0.345  0.634  0.021
-->
The posterior probabilities differs from the confirmatory test because the complement hypothesis, $\mu<5$, is also included in the exploratory test. In the exhaustive test the complement hypothesis receives the largest posterior probability. This can be explained from the fact that the unconstrained posterior is concentrated in the region where $\mu<5$ (Figure 1). Finally note that the posterior odds between the hypotheses for $\mu=5$ and $\mu>5$ are equal in the exploratory and the confirmatory test, $\frac{0.345}{0.021} = \frac{0.943}{0.057}$ (with a small difference due to rounding in the last step).



## 2. 2-way ANOVA for research on numerical judgement
(Multivariate) analysis of (co)variance ((M)AN(C)OVA) is performed when the interest is in testing group means using dummy group variables under a (multivariate) normal linear model, possibly by correcting for certain covariates. To perform Bayesian ANOVA using `BFpack`, first one needs to fit an ANOVA model with the `aov` function, and subsequently, the fitted model is fed to `BF`. The Bayes factors are based on generalized adjusted fractional Bayes methodology (Section A.1). By default exploratory tests are executed for the presence of main effects and interaction effects.

In this section, a 2-way ANOVA model is considered where the two factors have two levels,
\[
y_i = \mu + \delta_1 x_{1i} + \delta_2 x_{2i} + \delta_{12} x_{1i}x_{2i} + \epsilon_i\text{, where $\epsilon_i\sim N(0,\sigma^2)$},
\]
where $x_{1i}$ and $x_{2i}$ are dummy variables (0 or 1) for the first and second factor, respectively, so that $\delta_1$, $\delta_2$, and $\delta_{12}$ capture the main effect of the first factor, the main effect of the second factor, and the interaction effect of the two factors, respectively, and $\sigma^2$ is the common within groups variance, a nuisance parameter. For the exploratory test, the two main effects would be tested according to $H_{1}:\delta_1=0$ against $H_{2}:\delta_1\not =0$ (first factor) and $H_{1}:\delta_2=0$ against $H_{2}:\delta_2\not =0$ (second factor), and the interaction effect would be tested as $H_{1}:\delta_{12}=0$ against $H_{2}:\delta_{12}\not =0$.

In experiment '4a' on numerical judgments of participants reported by \cite{Janiszewski:2008}, the outcome variable was the amount by which the price for a television estimated by a participant differed from an anchor price (expressed by means of a $z$ score), and the two factors where (1) whether the anchor price was rounded, e.g., \$5000, or precise, e.g., \$4989 (`anchor` = `rounded` or `precise`, respectively); and (2) whether the participants received a suggestion that the estimated price is close to the anchor value or whether they did not receive this suggestion (`motivation` = `low` or `high`, respectively). An example of a question, with `anchor` = `rounded` and `motivation` = `low`, was: ''The retail price of a TV is \$5000 (rounded). The actual price is only slightly lower than the retail price. Can you guess the price?''. Alternatively, by changing '\$5000' to '\$4989' in the question a `precise` anchor price is obtained. By changing `slightly lower' to `lower' a question with a `high` motivation is obtained.

This $2 \times 2$ ANOVA design can be tested using `BFpack` as follows
```
aov1 <- aov(price ~ anchor * motivation, data = tvprices)
BF(aov1)
```
For an object of class \code{aov}, \pkg{BFpack} also provides the Bayes factors for the existence of the main effects and interactions effects in the exploratory tests
<!--
Bayesian hypothesis test
Type: Exploratory
Object: aov
Parameter: group means
Method: generalized adjusted fractional Bayes factor

Main effects:
           Pr(H0) Pr(H1)
anchor          0      1
motivation      0      1

Interaction effects:
                  Pr(H0) Pr(H1)
anchor:motivation  0.251  0.749
-->
The results show clear evidence that there there is a main effect for the `anchor` factor and a main effect for the `motivation` factor (with posterior probabilities of approximately 1). Furthermore, there is some evidence that there interaction effect between the two factors is present (with a posterior probability of 0.749). More data need to be collected in order to draw a more decisive conclusion regarding the existence of an interaction.



## 3. Testing group variances in neuropsychology
Besides (or in addition to) testing group means, there are also situations where the interest is in testing the heterogeneity across populations. Equality and order constraints can be tested between group variances $\sigma^2$ under normally distributed groups, i.e., $N(\mu_q,\sigma^2_q)$, under group $q$, for $q=1,\ldots,Q$, where the group means are treated as nuisance parameters. In `BFpack`, homogeneity of group variances is tested by default, i.e., $H_1:\sigma^2_1=\ldots=\sigma^2_Q$ versus $H_2:\text{ not $H_1$}$. Equality/order constrained hypotheses on group variances can be specified using the `hypothesis` argument. For technical details we refer the interested reader to Section A.2.

\citet{Silverstein1995} conducted a psychological study to compare the attentional performances of 17 Tourette's syndrome (TS) patients, 17 ADHD patients, and 17 control subjects who did not suffer from TS or ADHD. The participants were shown a total of 120 sequences of either 3 or 12 letters. Each sequence contained either the letter T or the letter F at a random position. Each sequence was presented for 55 milliseconds and afterwards the participants had to indicate as quickly as possible whether the shown sequence contained a T or an F. After a participant completed all 120 sequences, his or her accuracy was calculated as the percentage of correct answers. In this section, we are interested in comparing the variances of the accuracies in the three groups. Research has shown that ADHD patients tend to be more variable in their attentional performances than subjects who do not suffer from ADHD \citep[e.g.,][]{Kofler2013, Russell2006}.  It is less well documented whether TS patients are less or more variable in their attentional performances than healthy control subjects. We will therefore test the following set of hypotheses to investigate whether TS patients are as variable in their attentional performances as either ADHD patients or healthy controls (C): $H_1\colon \sigma_C^2 = \sigma_{TS}^2 < \sigma_{ADHD}^2$ and $H_2\colon \sigma_C^2 < \sigma_{TS}^2 = \sigma_{ADHD}^2$. We will test these hypotheses against the standard hypothesis of equality of variances, $H_3\colon \sigma_C^2 = \sigma_{TS}^2 = \sigma_{ADHD}^2$, as well as the complement of the three aforementioned hypotheses given by $H_4\colon \text{ not }H_1, H_2, H_3$. The complement is included to safeguard against the data supporting neither of the first three constrained hypotheses.

\citet{Silverstein1995} reported the following sample variances of the accuracies in the three groups: $s_C^2 = 15.52$, $s_{TS}^2 = 20.07$, and $s_{ADHD}^2 = 38.81$. The data are contained in a dataset called \verb|attention|. In `BFpack`, we can conduct the multiple hypothesis test and weigh the evidence in favor of the four hypotheses as follows:
```
bartlett <- bartlett_test(x = attention$accuracy, g = attention$group)
hypothesis <- "Controls = TS < ADHD; Controls < TS = ADHD;
  Controls = TS = ADHD"
set.seed(358)
BF_var <- BF(bartlett, hypothesis)
```
Note that we use equal prior probabilities of the hypotheses by omitting the `prior` argument in the call to the `BF` function. The exploratory posterior probabilities for homogeneity of group variances can be obtained by running `summary(BF_var)` which yields
<!--
Bayesian hypothesis test
Type: Exploratory
Object: BF_bartlett
Parameter: group variances
Method: generalized adjusted fractional Bayes factor

   homogeneity of variances no homogeneity of variances 
                      0.803                       0.197
-->
This results in evidence for equality of group variances. Note that the $p$ value in the classical Bartlett test for these data equals 0.1638 which implies that the null hypothesis of homogeneity of variances cannot be rejected using common significance levels, such as 0.05 or 0.01. Note however that this $p$ value cannot be used as a measure for the evidence in the data in favor of homogeneity of group variances. This can be done using the proposed Bayes factor test which shows that the probability that the variances are equal is approximately 0.803.

The confirmatory test provides a more detailed analysis about the most plausible relationship between the hypotheses (also obtained using the `summary()` call):
<!--
Bayesian hypothesis test
Type: Confirmatory
Object: BF_bartlett
Parameter: group variances
Method: generalized adjusted fractional Bayes factor

Posterior probabilities:
   Pr(hypothesis|data)
H1               0.426
H2               0.278
H3               0.238
H4               0.058

Hypotheses:
H1: Controls=TS<ADHD
H2: Controls<TS=ADHD
H3: Controls=TS=ADHD
H4: complement
-->
Thus, $H_1$ receives strongest support from the data, but $H_2$ and $H_3$ are viable competitors. It appears that even the complement $H_3$ cannot be ruled out entirely given a posterior probability of 0.058. To conclude, the results indicate that TS population are as heterogeneous in their attentional performances as the healthy control population in this specific task, but further research would be required to obtain more conclusive evidence.



## 4. Logistic regression in forensic psychology
The generalized linear model is a flexible generalization of the normal linear regression model where the outcome variable has an error that follows a nonnormal distribution \citep{McCullagh}. The logistic regression model is one of the most commonly used generalized linear models where the outcome variable is a binary variable. A logit function of the ''success'' probability of the outcome variable is assumed to follow a linear function of the predictor variables, $\beta_1x_{i1}+\ldots+\beta_Qx_{iq}$. For a fitted `glm` model, the `BF` function executes a Bayes factor test based on the approximate fractional Bayes using a minimal fraction \citep{Gu:2017}. For technical details we refer the interested reader to Section A.3. Here we consider an application from forensic psychology.

The presence of systematic biases in the legal system runs counter to society's expectation of fairness. Moreover such biases can have profound personal ramifications, and the topic therefore warrants close scrutiny. \cite{Wilson:2015} examined the correlation between perceived facial trustworthiness and criminal-sentencing outcomes (data available at \url{https://osf.io/7mazn/}). In Study 1 photos of inmates who had been sentenced to death (or not) were rated by different groups of participants on trustworthiness, 'Afrocentricity' (how sterotypically `black' participants were perceived as), attractiveness and facial maturity. Each photo was also coded for the presence of glasses/tattoos and facial width-to-height ratio. A logistic regression with sentencing as outcome was fitted to the predictors.

Previous research had shown that the facial width-to-height ratio (fWHR) has a postive effect on perceived aggression and thus may also have a positive effect on sentencing outcomes. In addition, perceived Afrocentricity had been shown to be associated with harsher sentences \citep[][]{Wilson:2015}. In the first hypothesis it was expected that all three predictors have a positive effect on the probability of being sentenced to death. Additionally, we might expect lack of perceived trustworthiness to have the largest effect. In the second hypothesis it was assumed that only trustworthiness has a positive effect. Finally, the complement hypothesis was considered. The hypotheses can then be summarized as follows
\begin{eqnarray*}
H_1&:& \beta_{trust} > (\beta_{fWHR}, \beta_{afro}) > 0\\
H_2&:& \beta_{trust} > \beta_{fWHR} = \beta_{afro} = 0\\
H_3&:& \text{neither $H_1$, nor $H_2$.}
\end{eqnarray*}

Before fitting the logistic regression we reverse-coded the trustworthiness scale and standardized it to be able to compare the magnitude the three effects. We can then test these hypotheses using `BFpack` and the fitted `glm` object from `R`. Note that the fitted object also contains covariates. The full logistic regression model was first fitted, and then the above hypotheses were tested on the fitted `glm` object:
<!--
fit <- glm(sent ~ ztrust + zfWHR + zAfro + glasses + attract +
  maturity + tattoos, family = binomial(), data = wilson)
set.seed(123)
BF_glm <- BF(fit, hypothesis = "ztrust > (zfWHR, zAfro) > 0;
  ztrust > zfWHR = zAfro = 0")
summary(BF_glm)
-->

In the output we see little support for the first two hypotheses; the complement receives most support:
<!--
Posterior probabilities:
   Pr(hypothesis|data)
H1               0.071
H2               0.002
H3               0.927

Evidence matrix:
       H1      H2    H3
H1  1.000  33.066 0.076
H2  0.030   1.000 0.002
H3 13.133 434.246 1.000

Hypotheses:
H1: ztrust>(zfWHR,zAfro)>0
H2: ztrust>zfWHR=zAfro=0
H3: complement
-->
The evidence matrix shows that the complement hypothesis is around 11.755 times as likely as the second best hypothesis. Based on these results we see that the complement receives most evidence. The fact that none of the two anticipated hypotheses were supported by the data indicates that the theory is not yet well-developed. Closer inspection of the beta-coefficients reveals that this is largely driven by the negative effect between perceived Afrocentricity and sentencing harshness ($\hat{\beta}_{zAfro}=-0.18071$). This unexpected result is discussed further by \cite{Wilson:2015} in their Supplementary Materials (\url{https://journals.sagepub.com/doi/suppl/10.1177/0956797615590992}).

Finally we illustrate how an exploratory Bayes factor test can be executed from output of a classical significance test using the `BF.default` (Section \ref{sect3}):
```
ct <- lmtest::coeftest(fit)
BF(ct[,1], Sigma = diag(ct[,2]^2), n = nrow(wilson))
```
Note that for the exploratory tests of the separate effects only the separate standard errors are needed, and not the entire error covariance matrix (which is also not contained in the `ct` object), because for every separate tests the remaining parameters are treated as nuisance parameters and are thus integrated out. Thus the parameters cannot be tested against each other based on the output of the function `coeftest`. The output yields
<!--
Call:
BF.default(x = ct[, 1], Sigma = diag(ct[, 2]^2), n = nrow(wilson))

Bayesian hypothesis test
Type: Exploratory
Object: numeric
Parameter: General
Method: Bayes factor using Gaussian approximations

Posterior probabilities:
            Pr(=0) Pr(<0) Pr(>0)
(Intercept)  0.853  0.014  0.133
ztrust       0.000  0.000  1.000
zfWHR        0.001  0.000  0.999
zAfro        0.365  0.631  0.004
glasses      0.712  0.009  0.278
attract      0.930  0.041  0.029
maturity     0.770  0.219  0.011
tattoos      0.787  0.011  0.202
-->
Because the sample is equal across these tests, the posterior probabilities for zero effects are proportional to the two-tailed p-values. Classical $p$ values however tend to overestimate the evidence against the null hypothesis \cite{Sellke:2001}. This can for instance be seen from the two-tailed $p$ value of `zAfro`, which is equal to .01188 (which is the fourth element in the output of `ct[,4]`), while the posterior probability that the effect of `zAfro` is zero is equal to .365 (based on a prior probability of $\frac{1}{3}$ in the exploratory test). This is one of the motivations for the recent call for using more conservative significance levels when interpreting $p$ values \citep{Benjamin:2017}.


## 5. Multivariate linear regression for fMRI studies
Multivariate normal linear regression models are useful for better understanding how a set of $K$ predictor variables affect $P$ outcome variances when the error follows a multivariate normal error with unknown covariance matrix:
\[
y_{ip} = \mu_p + \beta_{1p}x_{i1} + \ldots + \beta_{Kp}x_{iK} + \epsilon_{ip},
\]
for the $p$-th outcome variable, for $p=1,\ldots,P$, where $(\epsilon_{i1},\ldots,\epsilon_{iP})'\sim N(\textbf{0},\boldsymbol\Sigma)$, where $\boldsymbol\Sigma$ is an unknown error covariance matrix, and where $\beta_{kp}$ denotes the effect of the $k$-th predictor variable on the $p$-th outcome variable. First `lm` can be used to fit the multivariate normal linear regression model for a given data set. Subsequently Bayesian hypothesis tests are executed using generalized adjusted fractional Bayes factors using `BFpack`(Section A.1). By running the `get_estimates()` function on the multivariate `lm` object ('`mlm`'), a vector is obtained containing the parameters and corresponding labels on which hypotheses can be formulated using the `hypothesis` argument. For example, if a predictor variable has name `x1` and a dependent variable has name `y1`, then the effect of the predictor variable on the dependent variable is labeled as `x1_on_y1`. Hypotheses can be tested by formulating constraints on the effects on the same dependent variable or on effect across different dependent variables. In this section an application is considered from fMRI studies.

It is well established that the fusiform facial area (FFA), located in the inferior temporal cortex of the brain, plays an important role in the recognition of faces. This data comes from a study on the association between the thickness of specific cortical layers of the FFA and individual differences in the ability to recognize faces and vehicles \citep{McGuigin:2019}. High-resolution fMRI was recorded from 13 adult participants, after which the thickness of the superficial, middle, and deep layers of the FFA was quantified for each individual. In addition, individual differences in  face and vehicle recognition ability were assessed using a battery of tests.

### Analysis of the complete data
In this example, two alternative hypotheses are tested.  In a recent study, \cite{McGuigin:2016} found that individual differences in the overall thickness of the FFA are negative correlated with the ability to recognize faces but positively correlated with the ability to recognize cars. $(H_{1})$ is the most parsimonious extension of these findings. It specifies that the magnitude and direction of the association between object recognition and layer thickness is not moderated by layer. To elaborate, consider a multivariate multiple regression model model with cortical thickness measures for the superficial, middle, and deep layers as three repeated (dependent) measures for each participant %[and layer (a factor with three levels) DELETE????]
, and facial recognition ability and vehicle recognition ability as two dependent variables. Hypothesis $H_{1}$ is a main effects only model specifying that only main effect terms for face and vehicle are sufficient to predict the thickness of layers. The absence of layer $\times$ face or layer $\times$ vehicle interaction terms means that the relations between face and vehicle recognition are invariant across cortical layers. In other words, this hypothesis specifies that:
\begin{eqnarray*}
H_1&:&\beta_{Deep\_on\_Face} = \beta_{Middle\_on\_Face} = \beta_{Superficial\_on\_Face} < 0 <
\beta_{Deep\_on\_Vehicle}\\
&&= \beta_{Middle\_on\_Vehicle} = \beta_{Superficial\_on\_Vehicle}.
\end{eqnarray*}
That is, regression coefficients between face recognition and cortical thickness measures are expected to be negative, coefficients between vehicle recognition and cortical thickness measures are expected to be positive, and no layer-specific effect is expected for either faces or vehicles.

Hypothesis $H_{2}$ is based on prior findings concerning the early development of facial recognition abilities and the more rapid development of the deep layer of the FFA. This evidence leads to the following hypothesis: 
\begin{eqnarray*}
H_2&:&\beta_{Deep\_on\_Face} < \beta_{Middle\_on\_Face} =\beta_{Superficial\_on\_Face} < 0 < \beta_{Deep\_on\_Vehicle} \\
&&=\beta_{Middle\_on\_Vehicle} =\beta_{Superficial\_on\_Vehicle} 
\end{eqnarray*}
That is, the negative effect between facial recognition and the cortical thickness would be more pronounced in the deep layer, relative to the superficial and middle layers. One could attempt to test and compare these two hypotheses using linear mixed effects models software (e.g., the `gls` function in the `lme` package in `R`) with an appropriate covariance structure on the residuals to account for within-subject dependence. Alternatively one could use a model selection framework like that embodied in the `BayesFactor` package in `R`. Unfortunately, while these approaches can test some components of each hypothesis, they are not well suited to test the directional component of $H_1$, which specifies that all coefficients involving faces are smaller than 0 and that all coefficients involving vehicles are larger than 0. This hypothesis can, however, be tested using `BFpack` in the following way. First a multivariate model is fitted with dependent variables `Superficial`, `Middle`, and `Deep` and predictor variables `Face` and `Vehicle`. Subsequently the fitted model and the constrained hypotheses on the effects (e.g., where `Face_on_Deep` refers to the effect of `Deep` on `Face`) are plugged into the `BF` function:
```
fmri.lm <- lm(cbind(Superficial, Middle, Deep) ~ Face + Vehicle,
  data = fmri)
constraints.fmri <- "Face_on_Deep = Face_on_Superficial = Face_on_Middle
  < 0 < Vehicle_on_Deep = Vehicle_on_Superficial = Vehicle_on_Middle;
  Face_on_Deep < Face_on_Superficial = Face_on_Middle < 0 <
  Vehicle_on_Deep = Vehicle_on_Superficial = Vehicle_on_Middle"
set.seed(123)
BF_fmri <- BF(fmri.lm, hypothesis = constraints.fmri)
print(BF_fmri)
```
This results in the following posterior probabilities and evidence matrix:
<!--
Posterior probabilities:
   Pr(hypothesis|data)
H1               0.023
H2               0.975
H3               0.002

Evidence matrix:
       H1    H2     H3
H1  1.000 0.024  13.35
H2 42.391 1.000 565.93
H3  0.075 0.002   1.00
-->
In this analysis, hypothesis $H_3$ is the complement hypothesis which assumes that neither the constraints hold under $H_1$ nor the constraints under $H_2$. The evidence matrix reveals there is clear evidence for $H_{2}$ against $H_1$ ($B_{21} = 42.391$) and extreme evidence for $H_2$ against $H_3$ ($B_{23} = 565.93$). The same conclusion can be drawn when looking at the posterior probabilities for the hypotheses. Based on these result we would conclude that hypothesis $H_2$ receives most evidence and the Bayesian probability of drawing the wrong conclusion after observing the data would be relatively small, namely, 0.025.


### Analysis with missing observations
Missing data are ubiquitous in statistical practice. Properly handling missing data in model selection and hypothesis testing problems has not received a lot of attention in the literature, as the focus generally lies on estimation problems in the presence of missing observations. Appendix B discusses how to estimate the Bayes factors of the form of the extended Savage-Dickey density ratio in Equation 4 using the unconstrained posterior predictive distribution when the missings are missing at random \cite{LittleRubin2002}. The methodology is based on \cite{Hoijtink:2018b}.

Here we illustrate how Bayes factors can be obtained in the case of random missing observations in the fMRI data set. A slightly simpler constrained hypothesis test is considered to reduce computation time for illustrational purposes, given by
```
constraints.fmri2 <- 
  "Face_on_Deep = Face_on_Superficial = Face_on_Middle < 0;
  Face_on_Deep < Face_on_Superficial = Face_on_Middle < 0"
```
First the Bayes factors and posterior probabilities are obtained for this hypothesis test for the complete data set:
```
fmri.lm2 <- lm(cbind(Superficial,Middle,Deep) ~ Face +
  Vehicle, data = fmri)
BF.fmri2 <- BF(fmri.lm2, hypothesis = constraints.fmri2)
```
This results in posterior probabilities of 0.050, 0.927, and 0.023 for the two constrained hypotheses and the complement hypothesis, respectively. The Bayes factor of the most supported hypothesis ($H_2$) against the second most supported hypothesis ($H_1$) equals $B_{21}=18.443$.

Now 10 missing observations (out of 65 separate observations in total) are randomly created that are missing at random:
```
fmri_missing <- fmri
set.seed(1234)
for(i in 1:10){
  fmri_missing[sample(1:nrow(fmri), 1), sample(1:ncol(fmri), 1)] <- NA
}
```
This results in 7 rows with at least one missing observation. Therefore listwise deletion would leave us with only 6 complete observations (of the 13 rows in total).
Even though list-wise deletion is generally not recommended \citep{Rubin:1987,Rubin:1996}, for this illustration we compute the Bayes factors and posterior probabilities based on these 6 complete data observations to illustrate the loss of evidence as a result of list-wise deletion.
```
fmri_listdel <- fmri_missing[!is.na(apply(fmri_missing, 1, sum)),]
fmri.lm2_listdel <- lm(cbind(Superficial, Middle, Deep) ~ Face + Vehicle,
  data = fmri_listdel)
BF.fmri2_listdel <- BF(fmri.lm2_listdel, hypothesis = constraints.fmri2)
print(BF.fmri2_listdel)
```
This results in posterior probabilities of 0.010, 0.820, and 0.170 for the two constrained hypotheses and the complement hypothesis, respectively. As expected the evidence for the hypothesis $H_2$ which received most evidence in based on the complete data set, decreased from 0.927 to 0.820.

Next we illustrate that multiple imputation results a smaller loss in evidence because the partly observed cases are still used in the analysis. We first generate 500 imputed data sets using `mice` from the `mice` package \citep{mice:2019}, and then use `BF()` to get the measures of relative fit and relative complexity for the equality and order constraints for the three hypotheses. These are be obtained from the element `BFtable_confirmatory` of an object of class `BF`:
```
M <- 500
library(mice)
mice_fmri <- mice :: mice(data = fmri_missing, m = M, meth = c("norm",
  "norm", "norm", "norm", "norm"), diagnostics = F, printFlag = F)
relmeas_all <- matrix(unlist(lapply(1:M, function(m){
  fmri.lm_m <- lm(cbind(Superficial, Middle, Deep) ~ Face + Vehicle,
  data = mice::complete(mice_fmri, m))
  BF.fmri2_m <- BF(fmri.lm_m, hypothesis = constraints.fmri2)
  c(BF.fmri2_m$BFtable_confirmatory[, 1:4])
})),ncol = M)
relmeas <- matrix(apply(relmeas_all, 1, mean),nrow = 3)
row.names(relmeas) <- c("H1", "H2", "H3")
colnames(relmeas) <- c("comp_E", "comp_O", "fit_E", "fit_O")
BF_tu_confirmatory <- relmeas[,3] * relmeas[,4] / (relmeas[,1] *
  relmeas[,2])
PHP <- BF_tu_confirmatory / sum(BF_tu_confirmatory)
print(PHP)
```
In the 11th line the averages are computed for the four different measures of relative fit and relative complexity for the constrained hypotheses in Equation 4, across all 100 imputed datasets. Appropriate names are given on lines 12 and 13 for illustrative purposes. Subsequently in the 14th line, the Bayes factors of all constrained hypotheses against an unconstrained alternative are computed using the Equation 4 based on the four different quantities (which also holds for the generalized adjusted fractional Bayes factor; Section A.1).
This results in posterior probabilities of 0.066, 0.909, and 0.025 for the two constrained hypotheses and the complement hypothesis, respectively. Thus the posterior probability for the most supported hypothesis is still 0.909 (compared to 0.927 based on the complete data set), which is considerably larger than the posterior probability of 0.820 which was obtained using the data set after list-wise deletion.



## 6. Testing measures of association in neuropsychology
Measures of association play a central role in the applied sciences to quantify the degree of association between the variables of interest, possibly while correcting for certain control variables. The Pearson product-moment correlation coefficient is the most commonly used measure of association which expresses the strength of the linear relationship between two continuous variables. `BFpack` allows researchers to perform simple Bayesian tests on single correlations, e.g., $H_1:\rho=0$ versus $H_2:\rho<0$ versus $H_3:\rho>0$) and complex Bayesian tests of equality and order constraints on multiple correlations (e.g., $H_1:\rho_1=\rho_2=\rho_3$ versus $H_2:\rho_1<\rho_2<\rho_3$ versus $H_3:\text{ not }H_1,~H_2$). First an unconstrained Bayesian model can be fitted for correlation analysis using the function `cor_test`. The fitted object can then be fed to `BF` to perform Bayes factor tests on the correlations. These Bayes factors are based on uniform priors for the free correlations under the constrained hypotheses. Technical details on the implemented Bayes factors for testing measures of association can be found in Section A.4. In this section the methodology is applied in the field of neuropsychology.

Schizophrenia is often conceptualized as a disorder of ``dysconnectivity'' characterized by disruption in neural circuits that connect different regions of the brain (e.g., Friston \& Firth, 1995). This data set (originally collected by Ichinose, Han, Polyn, Park  and Tomarken (2019; summarized in Tomarken \& Mulder, in preparation) can be used to test whether such dysconnection is manifested behaviorally as weaker correlations among measures that we would expect to be highly correlated among non-schizophrenic individuals. 20 patients suffering from schizophrenia (SZ group) and 20 healthy control (HC group) participants were administered six measures of working memory. Ichinose et al. hypothesized that each of the 15 correlations would be smaller in the schizophrenic group relative to the control group.

This data set is an interesting case of how an order-constrained Bayesian approach can provide a more powerful and more appropriate test relative to alternative methods. The table presents the Pearson correlations for the two groups. Several features are notable: (1) Each of the 15 correlations is higher in the HC group than the SZ group; (2) On average the correlations among the HC group are rather high (on average $0.59$); and, (3) The average correlation within the SZ group is essentially 0. Despite this clear pattern, there were significant differences between the HC and SZ groups on only 2 of 15 correlations when the false discovery rate was used to control for multiple testing. 

\begin{table}[ht]
\centering
\newcommand{\mysubscript}[1]{\raisebox{-0.34ex}{\scriptsize#1}}
\renewcommand\thetable{3}
\begin{tabular}{rrrrrrr}
  \hline
 & Im & Del & Wmn & Cat & Fas & Rat \\ 
  \hline
  Im &  & 0.35 & -0.07 & -0.28 & -0.17 & 0.08 \\ 
  Del & 0.83 & & -0.22 & 0.16 & 0.27 & 0.09 \\ 
  Wmn & 0.65 & 0.50 &  & -0.05 & 0.01 & -0.02 \\ 
  Cat & 0.56 & 0.39 & 0.77 & & 0.22 & -0.25 \\ 
  Fas & 0.39 & 0.32 & 0.70 & 0.73  & &-0.14 \\ 
  Rat & 0.54 & 0.47 & 0.61 & 0.77 & 0.67 & \\ 
   \hline
\end{tabular}\label{tablecorr}
\caption{Correlations for the SZ (above diagonal) and HC (below diagonal) groups.}
\end{table}

Given that the overall pattern of group differences is consistent with hypotheses, simultaneous testing procedures would appear to be a better approach than tests on individual correlations. Indeed, both maximum likelihood and resampling tests convincingly indicated that the covariance and correlation matrices across groups differ ($p < 0.01$). However, there are a number of ways in which two correlation or covariance matrices may differ. Thus, the conventional procedures for comparing matrices do not test the specific hypothesis that, for each of the 15 correlations, the value for the HC group is greater than the value for the SZ group.

This hypothesis can, however, be tested in a straightforward manner using `BFpack`. $H_{1}$ specifies that each correlation in the HC group is expected to be larger than the corresponding correlation in the SZ group (i.e., a total of 15 order constraints were imposed). The complement hypothesis $H_{2}$ represents any pattern of correlations other than those that were consistent with $H_1$. First the data `memory` is split into two for the two different groups, and the unconstrained posteriors for the (Fisher transformed) correlations are estimated using the `cor_test` function for the HC group (group 1) and the SZ group (group 2). The group numbers follow from the order that the data matrices are plugged into `cor_test`. Subsequently, the hypotheses are tested using `BF`:
```
memoryHC <- subset(memory,Group=="HC")[,-7]
memorySZ <- subset(memory,Group=="SZ")[,-7]
set.seed(123)
cor1 <- cor_test(memoryHC,memorySZ)
BF6_cor <- BF(cor1, hypothesis =
  "Del_with_Im_in_g1 > Del_with_Im_in_g2 &
  Del_with_Wmn_in_g1 > Del_with_Wmn_in_g2 & 
  Del_with_Cat_in_g1 > Del_with_Cat_in_g2 &
  Del_with_Fas_in_g1 > Del_with_Fas_in_g2 &
  Del_with_Rat_in_g1 > Del_with_Rat_in_g2 & 
  Im_with_Wmn_in_g1 > Im_with_Wmn_in_g2 & 
  Im_with_Cat_in_g1 > Im_with_Cat_in_g2 &
  Im_with_Fas_in_g1 > Im_with_Fas_in_g2 &
  Im_with_Rat_in_g1 > Im_with_Rat_in_g2 & 
  Wmn_with_Cat_in_g1 > Wmn_with_Cat_in_g2 &
  Wmn_with_Fas_in_g1 > Wmn_with_Fas_in_g2 &
  Wmn_with_Rat_in_g1 > Wmn_with_Rat_in_g2 & 
  Cat_with_Fas_in_g1 > Cat_with_Fas_in_g2 &
  Cat_with_Rat_in_g1 > Cat_with_Rat_in_g2 & 
  Fas_with_Rat_in_g1 > Fas_with_Rat_in_g2")
print(BF6_cor)
```
This yields:
<!--
Posterior probabilities:
   Pr(hypothesis|data)
H1                   1
H2                   0

Evidence matrix:
   H1       H2
H1  1 5647.244
H2  0    1.000
-->
Thus, the Bayes Factor for $H_{1}$ against $H_{2}$ is approximately $5647$ and the posterior probability for $H_{1}$ was effectively 1. Thus the order-constrained analysis indicate decisive support for the researchers' hypothesis. 


## 7. Intraclass correlations in educational testing
The multilevel or mixed effects model is the gold standard for modeling hierarchically structured data. In the mixed effects model the within-clusters variability is separately modeled from the between-clusters variability. The intraclass correlation plays a central role as a measure of the relative degree of clustering in the data where an intraclass correlation close to 1 (0) indicates a very high (low) degree of clustering in the data. Despite the widespread usage of mixed effects models in the (applied) statistical literature, there are few statistical tests for testing variance components; exceptions include \cite{Westfall:1996,Garcia:2007,Saville:2009,Thalmann:2017}. Recently, a Bayes factor for testing equality/order constrained hypotheses on intraclass correlations (and random intercept variances) was proposed by \cite{MulderFox:2019} under a marginal modeling framework \citep{Fox:2017,MulderFox:2013}. In the marginal model the random effects are integrated out and the intraclass correlations have become covariance parameters in a structured covariance matrix. As a consequence the intraclass correlations can attain negative values. A negative intraclass correlation implies that there is a smaller degree of clustering than under random group assignment. As the intraclass correlations are bounded proper uniform priors can be specified under the constrained hypotheses. For example under the unconstrained model, uniform priors are specified for the intraclass correlations in the interval $(-\frac{1}{p-1},1)$, where $p$ is the cluster size. This prior is equivalent to a shifted-$F$ prior on the between-cluster variances. Improper Jeffreys priors are used for the nuisance parameters $\boldsymbol\beta$ and $\phi^2$. By default, posterior probabilities are computed when testing whether a intraclass correlation is zero, negative, or positive. Technical details of the implemented model can be found in Section A.5.

Data from the Trends in International Mathematics and Science Study (TIMSS; \url{http://www.iea.nl/timss}) were used to examine differences in intraclass correlations of four countries (The Netherlands (NL), Croatia (HR), Germany (DE), and Denmark (DK)) with respect to the mathematics achievements of fourth graders (e.g., the first plausible value was used as a measure of mathematics achievement). The sample design of the TIMSS data set is known to describe three levels with students nested within classrooms/schools, and classrooms/schools nested within countries (e.g., one classroom is sampled per school). In this example, the TIMSS 2011 assessment was considered.

The intraclass correlation was defined as the correlation among measured mathematics achievements of grade-4 students attending the same school. This intraclass correlation was assumed to be homogenous across schools in the same country, but was allowed to be different across countries. For the four countries, differences in intraclass correlations were tested using the Bayes factor. The size of the intraclass correlation can be of specific interest, since sampling becomes less efficient when the intraclass correlation increases. Countries with low intraclass correlations have fewer restrictions on the sample design, where countries with high intraclass correlations require more efficient sample designs, larger sample sizes, or both. Knowledge about the size of the heterogeneity provide useful information to optimize the development of a suitable sample design and to minimize the effects of high intraclass correlations.

The TIMSS data sample in `BFpack` consists of four countries, where data was retrieved from The Netherlands ($93,112$), Croatia ($139, 106$), Germany ($179, 170$), and Denmark ($166, 153$) with the sampled number of schools in brackets for 2011 and 2015, respectively. Differences in intraclass correlations were tested conditional on several student variables (e.g., gender, student sampling weight variable). The following hypotheses on intraclass correlations were considered in the analyses. Country-ordered intraclass correlations were considered by hypothesis $H_1$, equal (invariant) intra-class correlations were represented by hypothesis $H_2$, and their complement was specified as hypothesis $H_3$:
\begin{eqnarray*}
H_1 &:& \rho_{NL} < \rho_{HR} < \rho_{DE} < \rho_{DK} \\
H_2 &:& \rho_{NL} = \rho_{HR} = \rho_{DE} = \rho_{DK} \\
H_3 &:& \text{neither $H_1$, nor $H_2$.} 
\end{eqnarray*}
The ordering in the intraclass correlations was hypothesized by considering the reported standard errors of the country-mean scores. From the variance inflation factor followed, $1+(p-1)\rho$, with $p$ the number of students in each school (balanced design), it follows that the variance of the mean increases for increasing values of the intraclass correlation coefficient. As a result, the ordering in estimated standard errors of the average mathematic achievements of fourth graders of the cycles from 2003 to 2015 was used to hypothesis the order in intraclass correlations. From a more substantive perspective, it is expected that schools in the Netherlands do not differ much with respect to their performances (low intraclass correlation) in contrast to Denmark, where school performances may differ considerably (high intraclass correlation).

A linear mixed effects model was used to obtain (restricted) maximum likelihood estimates of the fixed effects of the student variables and the country means, the four random effects corresponding to the clustering of students in schools in each country, and the measurement error variance, given the 2011 assessment data.
```
R> library(lme4)
R> timssICC_subset <- subset(timssICC, groupNL11 == 1 | groupHR11 == 1 | 
+    groupDE11 == 1 | groupDK11 == 1)
R> outlme1 <- lmer(math ~ -1 + gender + weight + lln +
+    groupNL11 + (0 + groupNL11 | schoolID) +
+    groupHR11 + (0 + groupHR11 | schoolID) +
+    groupDE11 + (0 + groupDE11 | schoolID) +
+    groupDK11 + (0 + groupDK11 | schoolID), 
+    data=timssICC_subset)
```
where the `schoolID` factor variable assigns a unique code to each school, and each country-specific group variable (e.g., `groupNL11`) equals one when it concerns a school in that country and zero otherwise. The `lmer` output object \citep{lme4} was used as input in the `BF` function for the Bayes factor computation, where hypothesis $H_1$ and $H_2$ were added as arguments in the function call;
```
R> set.seed(123)
R> BFicc <- BF(outlme1, hypothesis = 
+    "groupNL11 < groupHR11 < groupDE11 < groupDK11;
+    groupNL11 = groupHR11 = groupDE11 = groupDK11")
```
The output object contains the posterior mean and median estimates of the ICCs (obtained via `BFicc$estimates`), which are represented in Table \ref{tableICC}. The REML intraclass correlation estimates are also given for each country, which followed directly from the random effect estimates of the `lmer` output. It can be seen that the posterior mean and REML estimates essentially equal for these data.
	\begin{table}[!t]
		\centering
		\label{tableICC}
		\begin{tabular}{ccccc}
			\hline
			\multicolumn{1}{c}{Statistic} & \multicolumn{1}{c}{NL} & \multicolumn{1}{c}{HR} & \multicolumn{1}{c}{DE} & \multicolumn{1}{c}{DK} \\ \hline
			REML     & 0.099 & 0.122 & 0.156 & 0.195 \\
            Mean    & 0.099  &   0.126   &  0.159   &  0.198 \\
            Median & 0.098  &   0.124 &    0.158  &   0.198 \\
            2.5\%   & 0.061  &   0.091   &  0.123  &   0.157 \\
            97.5\% & 0.146   &  0.168    & 0.201   &  0.245 \\ \hline
		\end{tabular}
				\caption{TIMSS 2011: Intraclass correlation estimates for NL, HR, DE, and DK}
	\end{table}

By running \code{summary(BFicc)} we get the results of the exploratory and confirmatory tests. 
The exploratory tests provide posterior probabilities of whether each intraclass correlation equals zero, negative, or positive. Evidence in favor of a negative intraclass correlation indicates that a multilevel model may not be appropriate for modeling these data \citep{MulderFox:2019}. As can be seen the exploratory results indicate that a multilevel model is a appropriate for these data:
<!--
Bayesian hypothesis test
Type: Exploratory
Object: lmerMod
Parameter: intraclass correlations
Method: Bayes factor based on uniform priors

          icc=0 icc<0 icc>0
groupNL11     0     0     1
groupHR11     0     0     1
groupDE11     0     0     1
groupDK11     0     0     1
-->
Furthermore the posterior probabilities of the specified hypotheses shows how our beliefs are updated in light of the observed data regarding the hypotheses that were formulated on the variation of school performance across countries.
<!--
Bayesian hypothesis test
Type: Confirmatory
Object: lmerMod
Parameter: intraclass correlations
Method: Bayes factor based on uniform priors

Posterior probabilities:
   Pr(hypothesis|data)
H1               0.080
H2               0.890
H3               0.030

Hypotheses:
H1: groupNL11<groupHR11<groupDE11<groupDK11
H2: groupNL11=groupHR11=groupDE11=groupDK11
H3: complement
-->
The posterior probabilities of the three hypotheses in the confirmatory test reveal that the hypothesis of equal intraclass correlations receives most evidence from the data, followed by the order hypothesis, followed by the complement hypothesis, with posterior probabilities of .890, .080, and .030, respectively. These results indicate that there is evidence that an approximate equal and positive degree of clustering is present across countries.


## 8. Relational event models for analyzing communication networks
The relational event model (REM) allows researchers to investigate what mechanisms drive time-stamped relational events between actors in a social network \citep{Butts:2008}. The model can be seen as a cox proportional hazards model for the inter-event times based on time-varying covariates \citep{DuBois:2013}. The goal is to model the event rate between pairs of actors over time. Technical details of the model can be found in these two articles. The goal of the current section is to (i) illustrate how to test constrained hypotheses under REMs and (ii) illustrate how to use the default function of `BF` which only requires the unconstrained estimates of the parameters of interest, its error covariance matrix, and the sample size (see the second part of Section 3), in addition to the hypotheses.

In the current application, a simulated sequence of e-mail messages is analyzed based on the study of

\cite{MulderLeenders:2019}. This sample consists of 247 relational events in a network of 25 actors. It was investigated which mechanisms drive employees of a consultancy firm to send emails about innovation to each other, and to what degree. Homophily is often an important driver of relational events, which implies that individuals with similar attributes have an increased rate of interaction. Two attributes are considered in this context: (i) whether sender and receiver work in the same division, and (ii) whether sender and receiver have the same hierarchical position. Based on \cite{MulderLeenders:2019}, the following constrained hypotheses will be tested: 
\begin{align*}
\begin{split}
H_1&: \beta_\text{same division} = \beta_\text{same hierarchy} > 0\\
H_2&: \beta_\text{same division} > \beta_\text{same hierarchy} > 0\\
H_3&: \text{none of the above}
\end{split}
\end{align*}
Hypothesis $H_1$ assumes that the effects of working in the same division and having similar hierarchical position have an equal positive effect on the event rate. Hypothesis $H_2$ assumes that working in the same division has a larger effect than working in the same building, and both effects are positive. The complement hypothesis $H_3$ assumes that neither the constraints under $H_1$ nor the constraints under $H_2$ hold.

To test these hypotheses first the unconstrained REM is fit using the `R` package `relevent` \citep{relevent:2015} where the event rate between actor $s$ and $r$ at time $t$ follows a log linear function of the form
\begin{eqnarray*}
\log \lambda(s,r,t) &=& \beta_0 + \beta_{\text{inertia}}~ x_{\text{inertia}}(s,r,t) + \beta_\text{same division} ~x_\text{same division}(s,r)\\
&& + \beta_\text{same hierarchy}~x_\text{same hierarchy}(s,r)
\end{eqnarray*}
where $\beta_0$ is the intercept capturing the baseline of the event rate, $\beta_{\text{inertia}}$ is the inertia effect (i.e., the general tendency for actors to keep sending messages to actors who they sent messages to in the past), $x_{inertia}(s,r,t)$ is the number of past events from $s$ to $r$ until time $t$, and $x_\text{same division}(s,r)$ and $x_\text{same hierarchy}(s,r)$ are dichotomous variables whether actor $s$ and $r$ work in the same division (1=yes, 0=no) and whether actor $s$ and $r$ have the same hierarchical level (1=yes, 0=no). Given the hypotheses, the intercept and the inertia effect are nuisance parameters in the current application. The model can be fit as follows
```
library(relevent)
CovEventEff <- array(NA, dim = c(3, nrow(actors), nrow(actors)))
CovEventEff[1,,] <- 1
CovEventEff[2,,] <- as.matrix(same_division)
CovEventEff[3,,] <- as.matrix(same_hierarchy)
dimnames(CovEventEff)[[1]] <- c("baseline", "division", "hierarchy")
set.seed(9227)
remfit <- rem.dyad(edgelist = relevents, n = nrow(actors), effects = 
  c("FrPSndSnd", "CovEvent"), covar = list(CovEvent = CovEventEff),
  hessian = TRUE, fit.method = "BPM")
```

Bayes factors and posterior model probabilities for the evaluation of these informative hypotheses can be obtained with `BFpack` by running the following lines of code:
```
names(remfit$coef) <- c("inertia","baseline","division", "hierarchy")
hypothesis <- "division = hierarchy > 0; division > hierarchy > 0"
estimates <- remfit$coef
covmatrix <- remfit$cov
samplesize <- remfit$m
BFrem <- BF(estimates, Sigma = covmatrix, n = samplesize, 
  hypothesis = hypothesis)
print(BFrem)
```
In the first line new names are given to the estimated values with a clearer interpretation. These names are then used for formulating constrained hypotheses in the `hypothesis` argument. The following output is then obtained:
<!--
Call:
BF.default(x = estimates, hypothesis = hypothesis, Sigma = covmatrix, 
    n = samplesize)

Bayesian hypothesis test
Type: Confirmatory
Object: numeric
Parameter: General
Method: Bayes factor using Gaussian approximations

Posterior probabilities:
   Pr(hypothesis|data)
H1               0.000
H2               0.913
H3               0.087

Evidence matrix:
         H1    H2     H3
H1    1.000 0.000  0.002
H2 6070.727 1.000 10.534
H3  576.305 0.095  1.000

Hypotheses:
H1: division=hierarchy>0
H2: division>hierarchy>0
H3: complement
-->
The Bayes factors and posterior probabilities reveal there is most evidence for $H_2$ (with a posterior probability of 0.913), followed by the complement hypothesis $H_3$ (with a posterior probability of 0.087), and finally hypothesis $H_1$ received a posterior probability of zero. This suggests that there is most support that working in the same division has a larger effect on information sharing than hierarchical similarity and that both effects are positive. There is still a probability of .087 that the complement may be true after observing the data. More data would be needed in order to draw more decisive conclusions.



\bibliography{refs_mulder}
